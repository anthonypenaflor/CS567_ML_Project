{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "essays = pd.read_csv('../llm-detect-ai-generated-text/train_essays.csv')\n",
    "prompts = pd.read_csv('../llm-detect-ai-generated-text/train_prompts.csv')\n",
    "\n",
    "# Merge essays with prompts based on 'prompt_id'\n",
    "data = essays.merge(prompts, on='prompt_id', how='left')\n",
    "data['combined_text'] = data['prompt_name'] + \" \" + data['instructions'] + \" \" + data['text']\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')  \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = TextDataset(train_data['combined_text'].tolist(), train_data['generated'].tolist(), tokenizer)\n",
    "val_dataset = TextDataset(val_data['combined_text'].tolist(), val_data['generated'].tolist(), tokenizer)\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm-detect-deberta/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/envs/llm-detect-deberta/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.04732897857142117, Train Accuracy: 0.9935483870967742\n",
      "Validation Loss: 0.0017261037913461526, Validation Accuracy: 1.0\n",
      "Epoch 2, Train Loss: 0.017934408784663725, Train Accuracy: 0.9975806451612903\n",
      "Validation Loss: 0.0010734709018530946, Validation Accuracy: 1.0\n",
      "Epoch 3, Train Loss: 0.017630333014418402, Train Accuracy: 0.9975806451612903\n",
      "Validation Loss: 0.0013947509578429163, Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device\n",
    "print(f'Using device: {device}')\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)  \n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 3  # Assuming 'num_epochs' = 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop with accuracy tracking\n",
    "for epoch in range(3):  # num_epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_examples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_train_correct += (preds == batch['labels']).sum().item()\n",
    "        total_train_examples += batch['labels'].size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy = total_train_correct / total_train_examples\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_val_correct += (preds == batch['labels']).sum().item()\n",
    "            total_val_examples += batch['labels'].size(0)\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val_examples\n",
    "    print(f'Validation Loss: {total_val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-detect-deberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
