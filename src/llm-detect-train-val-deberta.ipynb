{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "essays = pd.read_csv('../llm-detect-ai-generated-text/train_essays.csv')\n",
    "prompts = pd.read_csv('../llm-detect-ai-generated-text/train_prompts.csv')\n",
    "\n",
    "# Merge essays with prompts based on 'prompt_id'\n",
    "data = essays.merge(prompts, on='prompt_id', how='left')\n",
    "data['combined_text'] = data['prompt_name'] + \" \" + data['instructions'] + \" \" + data['text']\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')  \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = TextDataset(train_data['combined_text'].tolist(), train_data['generated'].tolist(), tokenizer)\n",
    "val_dataset = TextDataset(val_data['combined_text'].tolist(), val_data['generated'].tolist(), tokenizer)\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm-detect-deberta/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/envs/llm-detect-deberta/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.03849066208306246, Train Accuracy: 0.983402489626556\n",
      "Validation Loss: 0.0190664929407201, Validation Accuracy: 0.9975845410628019\n",
      "Epoch 2, Train Loss: 0.01433093804136796, Train Accuracy: 0.9979253112033195\n",
      "Validation Loss: 0.01777570607821242, Validation Accuracy: 0.9975845410628019\n",
      "Epoch 3, Train Loss: 0.00984539467868326, Train Accuracy: 0.9979253112033195\n",
      "Validation Loss: 0.017166739428760663, Validation Accuracy: 0.9975845410628019\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device\n",
    "print(f'Using device: {device}')\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)  \n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 3  # num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop with accuracy tracking\n",
    "for epoch in range(3):  # num_epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_examples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_train_correct += (preds == batch['labels']).sum().item()\n",
    "        total_train_examples += batch['labels'].size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy = total_train_correct / total_train_examples\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_val_correct += (preds == batch['labels']).sum().item()\n",
    "            total_val_examples += batch['labels'].size(0)\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val_examples\n",
    "    print(f'Validation Loss: {total_val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model for paraphrasing\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_model.eval()\n",
    "t5_model.to(device)\n",
    "\n",
    "def paraphrase_text(texts, t5_model, tokenizer, max_length=200, batch_size=8):\n",
    "    \"\"\" Function to generate paraphrased text using T5 in batches \"\"\"\n",
    "    paraphrased_texts = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch_texts, max_length=max_length, return_tensors=\"pt\").input_ids.to(t5_model.device)\n",
    "        paraphrased_ids = t5_model.generate(input_ids, max_length=max_length, num_return_sequences=1, num_beams=2)\n",
    "        for j, g in enumerate(paraphrased_ids):\n",
    "            try:\n",
    "                paraphrased_text = tokenizer.decode(g, skip_special_tokens=True)\n",
    "                if not paraphrased_text:  # Check if paraphrased text is empty\n",
    "                    raise ValueError(\"Empty paraphrase generated\")\n",
    "            except:\n",
    "                paraphrased_text = batch_texts[j]  # Fallback to the original text if paraphrasing fails\n",
    "            paraphrased_texts.append(paraphrased_text)\n",
    "    return paraphrased_texts\n",
    "\n",
    "def paraphrase_and_prepare_data(validation_data, model, tokenizer):\n",
    "    \"\"\" Function to paraphrase and prepare data for DataLoader \"\"\"\n",
    "    \n",
    "    # Ensure text entries are valid strings\n",
    "    val_data_reset = validation_data.reset_index(drop=True)\n",
    "    \n",
    "    # Paraphrase the 'text' part of the validation data\n",
    "    paraphrased_texts = paraphrase_text(validation_data['text'].tolist(), model, tokenizer)\n",
    "\n",
    "    # Add paraphrased text as a new column in the validation data\n",
    "    paraphrased_texts_df = pd.DataFrame({'text': paraphrased_texts})\n",
    "\n",
    "    # Combine the paraphrased text with the prompt and instructions\n",
    "    val_data_reset['paraphrased_text'] = val_data_reset['prompt_name'].fillna('').str.cat(paraphrased_texts_df['text'].fillna(''), sep=' ')\n",
    "    \n",
    "    return val_data_reset\n",
    "\n",
    "# Apply paraphrasing and preparation to the validation data\n",
    "# paraphrased_val_data = paraphrase_and_prepare_data(val_data, t5_model, t5_tokenizer)\n",
    "\n",
    "# # Use your existing TextDataset class for the paraphrased data\n",
    "# paraphrased_val_dataset = TextDataset(paraphrased_val_data['paraphrased_text'].tolist(), paraphrased_val_data['generated'].tolist(), tokenizer)\n",
    "\n",
    "# # DataLoader for the paraphrased validation dataset\n",
    "# paraphrased_val_loader = DataLoader(paraphrased_val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "paraphrased_val_data = paraphrase_and_prepare_data(val_data, t5_model, t5_tokenizer)\n",
    "paraphrased_val_dataset = TextDataset(paraphrased_val_data['paraphrased_text'].tolist(), paraphrased_val_data['generated'].tolist(), tokenizer)\n",
    "paraphrased_val_loader = DataLoader(paraphrased_val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Paraphrased Data: 0.9975845410628019\n"
     ]
    }
   ],
   "source": [
    "# Evaluate existing dataset against adversarial attacks\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_correct += (preds == batch['labels']).sum().item()\n",
    "            total_examples += batch['labels'].size(0)\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate your DeBERTa model on the paraphrased validation data\n",
    "accuracy = evaluate_model(model, paraphrased_val_loader)\n",
    "print(f'Accuracy on Paraphrased Data: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-detect-deberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
