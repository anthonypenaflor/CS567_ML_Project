{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "# essays = pd.read_csv('../llm-detect-ai-generated-text/train_essays.csv')\n",
    "# prompts = pd.read_csv('../llm-detect-ai-generated-text/train_prompts.csv')\n",
    "# xsum_data = pd.read_csv('../additional_data/xsum/xsum-gen-model=mistral-7b-v0.1-n=5000.csv')\n",
    "# xsum_data = pd.read_csv('../additional_data/xsum/xsum-gen-n=5000-model=llama3-8b.csv')\n",
    "# xsum_data.dropna(inplace=True)\n",
    "# xsum_data.reset_index(drop=True, inplace=True)\n",
    "# hewlett_data = pd.read_csv('../additional_data/hewlett/hewlett-n=1000-instruct=False-model=mistral-7b-instruct.csv')\n",
    "# hewlett_data.dropna(inplace=True)\n",
    "# hewlett_data.reset_index(drop=True, inplace=True)\n",
    "daigt_data = pd.read_csv('../additional_data/daigt_full_dataset.csv')\n",
    "daigt_data.dropna(inplace=True)\n",
    "daigt_data.reset_index(drop=True, inplace=True)\n",
    "# Limit daigt\n",
    "daigt_data = daigt_data[:10000]\n",
    "\n",
    "# Get name of dataset\n",
    "dataset_name = 'daigt_dataset'\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(daigt_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')  \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = TextDataset(train_data['text'].tolist(), train_data['generated'].tolist(), tokenizer)\n",
    "val_dataset = TextDataset(val_data['text'].tolist(), val_data['generated'].tolist(), tokenizer)\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device\n",
    "print(f'Using device: {device}')\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)  \n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 3  # num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Prediction data\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Training loop with accuracy tracking\n",
    "for epoch in range(1):  # num_epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_examples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_train_correct += (preds == batch['labels']).sum().item()\n",
    "        total_train_examples += batch['labels'].size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_accuracy = total_train_correct / total_train_examples\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, Train Accuracy: {train_accuracy}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_val_correct += (preds == batch['labels']).sum().item()\n",
    "            total_val_examples += batch['labels'].size(0)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val_examples\n",
    "    print(f'Validation Loss: {total_val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayConfusionMatrix(y_true, y_pred, dataset, aa=\"\"):\n",
    "    # Convert lists to NumPy arrays if not already\n",
    "    y_true = np.array(y_true) if isinstance(y_true, list) else y_true\n",
    "    y_pred = np.array(y_pred) if isinstance(y_pred, list) else y_pred\n",
    "\n",
    "    # Calculate F1 score only for binary classification\n",
    "    if y_pred.ndim == 1 and len(set(y_pred)) == 2:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "        print(\"F1 Score:\", f1)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        display_labels=[\"Not Generated\", \"Generated\"],\n",
    "        cmap=plt.cm.Blues\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix: {dataset}, F1 Score: {str(f1.round(2))}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../confusion_matrices/{dataset_name}{aa}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayConfusionMatrix(true_labels, predictions, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model for paraphrasing\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_model.eval()\n",
    "t5_model.to(device)\n",
    "\n",
    "def paraphrase_text(texts, t5_model, tokenizer, max_length=200, batch_size=4):\n",
    "    \"\"\" Function to generate paraphrased text using T5 in batches \"\"\"\n",
    "    paraphrased_texts = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch_texts, max_length=max_length, padding=True, return_tensors=\"pt\").input_ids.to(t5_model.device)\n",
    "        paraphrased_ids = t5_model.generate(input_ids, max_length=max_length, num_return_sequences=1, num_beams=2)\n",
    "        for j, g in enumerate(paraphrased_ids):\n",
    "            try:\n",
    "                paraphrased_text = tokenizer.decode(g, skip_special_tokens=True)\n",
    "                if not paraphrased_text:  # Check if paraphrased text is empty\n",
    "                    raise ValueError(\"Empty paraphrase generated\")\n",
    "            except:\n",
    "                paraphrased_text = batch_texts[j]  # Fallback to the original text if paraphrasing fails\n",
    "            paraphrased_texts.append(paraphrased_text)\n",
    "    return paraphrased_texts\n",
    "\n",
    "def paraphrase_and_prepare_data(validation_data, model, tokenizer):\n",
    "    \"\"\" Function to paraphrase and prepare data for DataLoader \"\"\"\n",
    "    \n",
    "    # Ensure text entries are valid strings\n",
    "    val_data_reset = validation_data.reset_index(drop=True)\n",
    "    \n",
    "    # Paraphrase the 'text' part of the validation data\n",
    "    paraphrased_texts = paraphrase_text(validation_data['text'].tolist(), model, tokenizer)\n",
    "\n",
    "    # Add paraphrased text as a new column in the validation data\n",
    "    paraphrased_texts_df = pd.DataFrame({'text': paraphrased_texts})\n",
    "\n",
    "    # # Combine the paraphrased text with the prompt and instructions\n",
    "    # val_data_reset['paraphrased_text'] = val_data_reset['prompt_name'].fillna('').str.cat(paraphrased_texts_df['text'].fillna(''), sep=' ')\n",
    "\n",
    "    # Add paraphrased text as a new column in the validation data without the prompt and instructions\n",
    "    val_data_reset['paraphrased_text'] = paraphrased_texts_df['text']\n",
    "    \n",
    "    return val_data_reset\n",
    "\n",
    "# Apply paraphrasing and preparation to the validation data\n",
    "# paraphrased_val_data = paraphrase_and_prepare_data(val_data, t5_model, t5_tokenizer)\n",
    "\n",
    "# # Use your existing TextDataset class for the paraphrased data\n",
    "# paraphrased_val_dataset = TextDataset(paraphrased_val_data['paraphrased_text'].tolist(), paraphrased_val_data['generated'].tolist(), tokenizer)\n",
    "\n",
    "# # DataLoader for the paraphrased validation dataset\n",
    "# paraphrased_val_loader = DataLoader(paraphrased_val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "paraphrased_val_data = paraphrase_and_prepare_data(val_data, t5_model, t5_tokenizer)\n",
    "paraphrased_val_dataset = TextDataset(paraphrased_val_data['paraphrased_text'].tolist(), paraphrased_val_data['generated'].tolist(), tokenizer)\n",
    "paraphrased_val_loader = DataLoader(paraphrased_val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data for the paraphrased validation\n",
    "predictions_aa = []\n",
    "true_labels_aa = []\n",
    "\n",
    "# Evaluate existing dataset against adversarial attacks\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_correct += (preds == batch['labels']).sum().item()\n",
    "            total_examples += batch['labels'].size(0)\n",
    "            predictions_aa.extend(preds.cpu().numpy())\n",
    "            true_labels_aa.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate your DeBERTa model on the paraphrased validation data\n",
    "accuracy = evaluate_model(model, gpt2_loader)\n",
    "print(f'Accuracy on Paraphrased Data: {accuracy}')\n",
    "conf_matrix_aa = confusion_matrix(true_labels_aa, predictions_aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayConfusionMatrix(true_labels_aa, predictions_aa, \"Test Adversarial Attacks\", \"_aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paraphrased data to a CSV file\n",
    "paraphrased_val_data.to_csv(f'../adversarial_data/{dataset_name}-paraphrased_val_data.csv', index=False)\n",
    "\n",
    "# Save the model\n",
    "# model.save_pretrained('deberta-base-xsum-mistral-7b-v0.1')\n",
    "\n",
    "# # Save the tokenizer\n",
    "# tokenizer.save_pretrained('deberta-base-xsum-mistral-7b-v0.1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-detect-deberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
