{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XSUM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XSUM Dataset found! Loading from path...\n"
     ]
    }
   ],
   "source": [
    "xsum_dirpath = \"data/xsum\"\n",
    "if os.path.exists(xsum_dirpath):\n",
    "    print(\"XSUM Dataset found! Loading from path...\")\n",
    "    dataset = load_from_disk(xsum_dirpath)\n",
    "else:\n",
    "    print(f\"XSUM Dataset not found! Downloading and saving to {xsum_dirpath}\")\n",
    "    dataset = load_dataset(\"EdinburghNLP/xsum\")\n",
    "    dataset.save_to_disk(xsum_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "seed = 42\n",
    "dataset = dataset.shuffle(seed=seed)\n",
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 5000 articles for now\n",
    "n_samples = 5000\n",
    "samples = dataset[\"train\"].select(range(n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load hewlett data (In progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/dafirebanks/projects/CS567_ML_Project/data/hewlett-foundation-data/training_set_rel3.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/io/excel/_base.py:490\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/io/excel/_base.py:1734\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1702\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m   1722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m        DataFrame from the passed in Excel file.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/io/excel/_base.py:765\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m    764\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[0;32m--> 765\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py:615\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[0;34m(self, sheet, convert_float, file_rows_needed)\u001b[0m\n\u001b[1;32m    613\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[1;32m    616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell, convert_float) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:79\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     75\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source()\n\u001b[1;32m     76\u001b[0m parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[1;32m     77\u001b[0m                          data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[1;32m     78\u001b[0m                          date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m max_row:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:144\u001b[0m, in \u001b[0;36mparse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/xml/etree/ElementTree.py:1253\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m     data \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/xml/etree/ElementTree.py:1323\u001b[0m, in \u001b[0;36mXMLPullParser.read_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m events:\n\u001b[1;32m   1322\u001b[0m     event \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m-> 1323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, \u001b[38;5;167;43;01mException\u001b[39;49;00m):\n\u001b[1;32m   1324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m event\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# filepath = \"/home/dafirebanks/projects/CS567_ML_Project/data/hewlett-foundation-data/training_set_rel3.xlsx\"\n",
    "# df = pd.read_excel(filepath)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map essay sets to the actual prompts:\n",
    "# set2prompt = {\n",
    "#     1: \"More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\",\n",
    "#     2: \"\"\"Censorship in the Libraries\n",
    "# \"All of us can think of a book that we hope none of our children or any other children have taken off the shelf. But if I have the right to remove that book from the shelf -- that work I abhor -- then you also have exactly the same right and so does everyone else. And then we have no books left on the shelf for any of us.\" --Katherine Paterson, Author\n",
    "# Write a persuasive essay to a newspaper reflecting your vies on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.\"\"\",\n",
    "#     3: \"\"\"Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining.\n",
    "# Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.\"\"\",\n",
    "#     4: \"\"\"We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\"\"\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set  essay_id                                              essay\n",
       "0          1         1  Dear local newspaper, I think effects computer...\n",
       "1          1         2  Dear @CAPS1 @CAPS2, I believe that using compu...\n",
       "2          1         3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...\n",
       "3          1         4  Dear Local Newspaper, @CAPS1 I have found that...\n",
       "4          1         5  Dear @LOCATION1, I know having computers has a..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Selecting only the columns that are needed: essay_set, essay_id and essay\n",
    "# df = df[[\"essay_set\", \"essay_id\", \"essay\"]]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out essays with illegible words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In Wales, councils are responsible for funding and overseeing schools. But in England, Mr Osborne's plan will mean local authorities will cease to have a role in providing education. Academies are directly funded by central government and head teachers have more freedom over admissions and to change the way the\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_prompts = [sample[\"document\"] for sample in samples]\n",
    "xsum_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In Wales, councils are responsible for funding and overseeing schools.\\nBut in England, Mr Osborne\\'s plan will mean local authorities will cease to have a role in providing education.\\nAcademies are directly funded by central government and head teachers have more freedom over admissions and to change the way the school works.\\nIt is a significant development in the continued divergence of schools systems on either side of Offa\\'s Dyke.\\nAnd although the Welsh Government will get extra cash to match the money for English schools to extend the school day, it can spend it on any devolved policy area.\\nMinisters have no plans to follow suit.\\nAt the moment, governing bodies are responsible for setting school hours and they need ministerial permission to make significant changes.\\nThere are already more than 2,000 secondary academies in England and its extension to all state schools is unlikely to shake the Welsh Government\\'s attachment to what they call a \"community, comprehensive model\" for schools.\\nIt rejects claims that freedom given to academies can help drive up standards, and it points to academy-free Scotland as the best performing school system in the UK.\\nEducation Minister Huw Lewis said there was \"very little evidence to suggest\" academies have a positive impact in driving up standards and Wales would not be following the model.\\n\"The Tories have wasted hundreds of millions of pounds on academies and free schools and as the Chancellor finalises his budget plans to slash vital services even further, he is committing them to wasting even more on a failing endeavour.\\n\"We have no plans to introduce the chaos and waste of academies and free schools here in Wales.\"\\nNone of the main parties in May\\'s Assembly election - including the Welsh Conservatives - have said they want to introduce academies in Wales.\\nOwen Hathway, NUT Cymru\\'s policy officer, called the academy plans for England \"scandalous.\".\\n\"There is no evidence that academies work, no evidence that they raise standards, no evidence that they offer better quality education and no evidence that they are what parents and communities want,\" he said.\\n\"Certainly a commitment to comprehensive education is something we would want, and indeed expect, all parties to hold firm to in their manifestos for the forthcoming Welsh election.\"\\nBut the Welsh and English schools systems are still linked by a joint arrangement for teachers\\' pay and conditions.\\nAcademies are not tied to these pay scales so in effect Wednesday\\'s announcement will take all English schools out of the system and raise questions about the viability of an England and Wales pay and conditions structure.\\nThere is already growing momentum for the devolution of teachers\\' pay and conditions.\\nOriginally sceptical, the Welsh Labour Government is now broadly in favour.\\nSome teaching unions remain opposed because of concern that Welsh teachers would end up being paid less than those in England.\\nMr Hathway said teachers were concerned it could lead to regional pay.\\n\"At the same time we do of course recognise that the issue of pay is already becoming a grey area due to the negative changes we see taking place in England,\" he said.\\nBut an even bigger difference between the schools landscape on either side of the border, appears to make separate arrangements for pay increasingly likely in future.',\n",
       " 5000)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_prompts[0], len(xsum_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # \"gpt2-medium\"\n",
    "download_dir = \"/home/dafirebanks/projects/dont-stop-prompting/models\"\n",
    "batch_size = 50\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a0ba28ca8643379a836ef484b01a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=download_dir).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=download_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Switch the padding side to the right - doesn't really matter because all texts should have at least 30 tokens, but this is to be consistent with the original code\n",
    "if tokenizer.padding_side == \"left\":\n",
    "    tokenizer.padding_side = \"right\"\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Taken from https://github.com/eric-mitchell/detect-gpt/blob/447d2ce8177004203f42d1da87d7f93a2e31ad52/run.py#L208 to ensure consistency\n",
    "\n",
    "\n",
    "def encode_and_truncate(texts, n_prompt_tokens):\n",
    "    \"\"\"Encode each text as a list of token ids, and truncate to the first n_prompt_tokens tokens.\n",
    "    Args:\n",
    "        texts: list[str]\n",
    "            List of texts to encode.\n",
    "        n_prompt_tokens: int\n",
    "            Number of tokens to keep for each text. If the text has fewer than this many tokens, it will be padded.\n",
    "    Returns:\n",
    "        dict[str, torch.Tensor]\n",
    "            Dictionary of tensors, with keys \"input_ids\", \"attention_mask\", \"token_type_ids\".\n",
    "    \"\"\"\n",
    "    all_encoded = tokenizer(texts, return_tensors=\"pt\",\n",
    "                            padding=True).to(DEVICE)\n",
    "    return {key: value[:, :n_prompt_tokens] for key, value in all_encoded.items()}\n",
    "\n",
    "\n",
    "def sample_from_model(texts, min_words=55, n_prompt_tokens=30):\n",
    "    \"\"\"Sample from the model using the provided texts as prompts, until each sample has at least min_words words.\n",
    "    Args:\n",
    "        texts: list[str]\n",
    "            List of texts to use as prompts.\n",
    "        min_words: int\n",
    "            Minimum number of words for each sample.\n",
    "        n_prompt_tokens: int\n",
    "            Number of tokens to use for each prompt.\n",
    "    Returns:\n",
    "        list[str]\n",
    "            List of samples generated from the model.\n",
    "    \"\"\"\n",
    "    all_encoded = encode_and_truncate(texts, n_prompt_tokens)\n",
    "    decoded = [\"\" for _ in range(len(texts))]\n",
    "    sampling_kwargs = {\n",
    "        \"max_length\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 0.96,\n",
    "        \"min_length\": 150,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # sample from the model until we get a sample with at least min_words words for each example\n",
    "    # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works\n",
    "    tries = 0\n",
    "    while (m := min(len(x.split()) for x in decoded)) < min_words:\n",
    "        if tries != 0:\n",
    "            print()\n",
    "            print(\n",
    "                f\"min words: {m}, needed {min_words}, regenerating (try {tries})\")\n",
    "\n",
    "        outputs = model.generate(**all_encoded, **sampling_kwargs)\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        tries += 1\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def trim_to_shorter_length(texta, textb):\n",
    "    # truncate to shorter of o and s\n",
    "    shorter_length = min(len(texta.split(\" \")), len(textb.split(\" \")))\n",
    "    texta = \" \".join(texta.split(\" \")[:shorter_length])\n",
    "    textb = \" \".join(textb.split(\" \")[:shorter_length])\n",
    "    return texta, textb\n",
    "\n",
    "\n",
    "def generate_samples(raw_data, batch_size):\n",
    "    \"\"\"\n",
    "    NOTE: Samples are truncated to the length of the shorter text. This is to ensure that the samples are comparable.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"original\": [],\n",
    "        \"sampled\": [],\n",
    "    }\n",
    "\n",
    "    for batch in range(len(raw_data) // batch_size):\n",
    "        print(\"Generating samples for batch\", batch,\n",
    "              \"of\", len(raw_data) // batch_size)\n",
    "        original_text = raw_data[batch * batch_size: (batch + 1) * batch_size]\n",
    "        sampled_text = sample_from_model(original_text, min_words=55)\n",
    "\n",
    "        for o, s in zip(original_text, sampled_text):\n",
    "            o, s = trim_to_shorter_length(o, s)\n",
    "\n",
    "            # add to the data\n",
    "            data[\"original\"].append(o)\n",
    "            data[\"sampled\"].append(s)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples for batch 0 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples for batch 1 of 100\n",
      "Generating samples for batch 2 of 100\n",
      "Generating samples for batch 3 of 100\n",
      "Generating samples for batch 4 of 100\n",
      "Generating samples for batch 5 of 100\n",
      "Generating samples for batch 6 of 100\n",
      "Generating samples for batch 7 of 100\n",
      "Generating samples for batch 8 of 100\n",
      "Generating samples for batch 9 of 100\n",
      "Generating samples for batch 10 of 100\n",
      "Generating samples for batch 11 of 100\n",
      "Generating samples for batch 12 of 100\n",
      "Generating samples for batch 13 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples for batch 14 of 100\n",
      "Generating samples for batch 15 of 100\n",
      "Generating samples for batch 16 of 100\n",
      "Generating samples for batch 17 of 100\n",
      "Generating samples for batch 18 of 100\n",
      "Generating samples for batch 19 of 100\n",
      "Generating samples for batch 20 of 100\n",
      "Generating samples for batch 21 of 100\n",
      "Generating samples for batch 22 of 100\n",
      "Generating samples for batch 23 of 100\n",
      "Generating samples for batch 24 of 100\n",
      "Generating samples for batch 25 of 100\n",
      "Generating samples for batch 26 of 100\n",
      "Generating samples for batch 27 of 100\n",
      "Generating samples for batch 28 of 100\n",
      "Generating samples for batch 29 of 100\n",
      "Generating samples for batch 30 of 100\n",
      "Generating samples for batch 31 of 100\n",
      "Generating samples for batch 32 of 100\n",
      "Generating samples for batch 33 of 100\n",
      "Generating samples for batch 34 of 100\n",
      "Generating samples for batch 35 of 100\n",
      "Generating samples for batch 36 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples for batch 37 of 100\n",
      "Generating samples for batch 38 of 100\n",
      "Generating samples for batch 39 of 100\n",
      "Generating samples for batch 40 of 100\n",
      "Generating samples for batch 41 of 100\n",
      "Generating samples for batch 42 of 100\n",
      "Generating samples for batch 43 of 100\n",
      "Generating samples for batch 44 of 100\n",
      "Generating samples for batch 45 of 100\n",
      "Generating samples for batch 46 of 100\n",
      "Generating samples for batch 47 of 100\n",
      "Generating samples for batch 48 of 100\n",
      "Generating samples for batch 49 of 100\n",
      "Generating samples for batch 50 of 100\n",
      "Generating samples for batch 51 of 100\n",
      "Generating samples for batch 52 of 100\n",
      "Generating samples for batch 53 of 100\n",
      "Generating samples for batch 54 of 100\n",
      "Generating samples for batch 55 of 100\n",
      "Generating samples for batch 56 of 100\n",
      "Generating samples for batch 57 of 100\n",
      "Generating samples for batch 58 of 100\n",
      "Generating samples for batch 59 of 100\n",
      "Generating samples for batch 60 of 100\n",
      "Generating samples for batch 61 of 100\n",
      "Generating samples for batch 62 of 100\n",
      "Generating samples for batch 63 of 100\n",
      "Generating samples for batch 64 of 100\n",
      "Generating samples for batch 65 of 100\n",
      "Generating samples for batch 66 of 100\n",
      "Generating samples for batch 67 of 100\n",
      "Generating samples for batch 68 of 100\n",
      "Generating samples for batch 69 of 100\n",
      "Generating samples for batch 70 of 100\n",
      "Generating samples for batch 71 of 100\n",
      "Generating samples for batch 72 of 100\n",
      "Generating samples for batch 73 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples for batch 74 of 100\n",
      "Generating samples for batch 75 of 100\n",
      "Generating samples for batch 76 of 100\n",
      "Generating samples for batch 77 of 100\n",
      "Generating samples for batch 78 of 100\n",
      "Generating samples for batch 79 of 100\n",
      "Generating samples for batch 80 of 100\n",
      "Generating samples for batch 81 of 100\n",
      "Generating samples for batch 82 of 100\n",
      "Generating samples for batch 83 of 100\n",
      "Generating samples for batch 84 of 100\n",
      "Generating samples for batch 85 of 100\n",
      "Generating samples for batch 86 of 100\n",
      "Generating samples for batch 87 of 100\n",
      "Generating samples for batch 88 of 100\n",
      "Generating samples for batch 89 of 100\n",
      "Generating samples for batch 90 of 100\n",
      "Generating samples for batch 91 of 100\n",
      "Generating samples for batch 92 of 100\n",
      "Generating samples for batch 93 of 100\n",
      "Generating samples for batch 94 of 100\n",
      "Generating samples for batch 95 of 100\n",
      "Generating samples for batch 96 of 100\n",
      "Generating samples for batch 97 of 100\n",
      "Generating samples for batch 98 of 100\n",
      "Generating samples for batch 99 of 100\n"
     ]
    }
   ],
   "source": [
    "test = generate_samples(xsum_prompts, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches to look into in case they may have weird padding: 13, 36, 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[\"sampled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jeremy Corbyn unusually had the better of Theresa May in Prime Minister's Questions, brandishing leaked texts across the despatch box, claiming evidence that the Tories had given Surrey a special deal to avoid the chance of a damaging 15% council tax rise in a Conservative safe haven.\\nThe council, and ministers, denied there had been any stitch-up.\\nBut hours later, the government admitted they had agreed, in theory, that Surrey County Council could, like several others, try out keeping all of the business rates they raise from 2018, which could plug the gaps in funding in future.\\nThat change is due to be in force across in England by 2020. Technically therefore, Surrey County Council has not been offered any additional funding. But the prospect of more flexibility over their own income in future could help fill the council's coffers, and seems to have eased some of their concerns.\\nBut\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some generated samples\n",
    "j = 20\n",
    "i = 13 * batch_size + j\n",
    "test[\"original\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeremy Corbyn unusually had the better of Theresa May in Prime Minister\\'s Questions, brandishing leaked texts across the despatch box to taunt the Prime Minister about her own Brexit plan.\\n\\nThe PM was forced to defend plans to pay a £40 billion Brexit bill and was under pressure from Labour to reveal the Government\\'s plan for future relations with the EU after Brexit.\\n\\nHowever, she was caught off guard by the release of documents that suggested that she believed a \"soft\" Brexit option was the only way to save the Union and \"avert\" a second Scottish independence referendum.\\n\\nThe documents were released by Labour as they questioned why the Government would choose an outcome which will \"damage economic prospects, endanger our industrial base and cut living standards for the many to line the pockets of the few\".\\n\\nOne document, a Downing Street briefing note for Mrs May, claimed a \"soft\"'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"sampled\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn into a jsonl format\n",
    "all_data = []\n",
    "for i in range(len(test[\"original\"])):\n",
    "    nongen = {\"text\": test[\"original\"][i], \"label\": 0}\n",
    "    gen = {\"text\": test[\"sampled\"][i], \"label\": 1}\n",
    "\n",
    "    all_data.append(nongen)\n",
    "    all_data.append(gen)\n",
    "\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Wales, councils are responsible for funding...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Wales, councils are responsible for funding...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Up to 100,000 youngsters will be eligible for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Up to 100,000 youngsters will be eligible for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middlesbrough and Brighton face each other on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  In Wales, councils are responsible for funding...      0\n",
       "1  In Wales, councils are responsible for funding...      1\n",
       "2  Up to 100,000 youngsters will be eligible for ...      0\n",
       "3  Up to 100,000 youngsters will be eligible for ...      1\n",
       "4  Middlesbrough and Brighton face each other on ...      0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize as a df\n",
    "store_dirpath = \"data\"\n",
    "df = pd.DataFrame.from_records(all_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Wales, councils are responsible for funding...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Wales, councils are responsible for funding...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Up to 100,000 youngsters will be eligible for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Up to 100,000 youngsters will be eligible for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middlesbrough and Brighton face each other on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  In Wales, councils are responsible for funding...      0\n",
       "1  In Wales, councils are responsible for funding...      1\n",
       "2  Up to 100,000 youngsters will be eligible for ...      0\n",
       "3  Up to 100,000 youngsters will be eligible for ...      1\n",
       "4  Middlesbrough and Brighton face each other on ...      0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Optionally split into train/test\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store\n",
    "train.to_json(f\"{store_dirpath}/xsum-gen-train.jsonl\", orient=\"records\", lines=True)\n",
    "test.to_json(f\"{store_dirpath}/xsum-gen-test.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
