{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-Context Learning with Llama3-8b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure to mount your Google drive if the data files are there and you're running this on Colab. Otherwise, you can specify the file paths directly in the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnsIVwzWwc-L",
        "outputId": "397b8f22-4c1f-48a6-a035-12b904b4d6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Authenticate with huggingface because some models like llama3 are gated\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTq7L-6RQh0_",
        "outputId": "fac952bd-4189-406b-e0f9-e9696cbad58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.4.2-cp310-cp310-manylinux1_x86_64.whl (67.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flash-attn\n",
            "  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cmake>=3.21 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.27.9)\n",
            "Collecting ninja (from vllm)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vllm) (2.31.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.40.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.19.1)\n",
            "Collecting fastapi (from vllm)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai (from vllm)\n",
            "  Downloading openai-1.26.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard] (from vllm)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.7.1)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting tiktoken==0.6.0 (from vllm)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lm-format-enforcer==0.9.8 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.9.8-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting outlines==0.0.34 (from vllm)\n",
            "  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from vllm) (4.11.0)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.14.0)\n",
            "Collecting ray>=2.9 (from vllm)\n",
            "  Downloading ray-2.20.0-cp310-cp310-manylinux2014_x86_64.whl (65.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-ml-py (from vllm)\n",
            "  Downloading nvidia_ml_py-12.550.52-py3-none-any.whl (39 kB)\n",
            "Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm)\n",
            "  Downloading vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==2.3.0 (from vllm)\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.26.post1 (from vllm)\n",
            "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting interegular>=0.3.2 (from lm-format-enforcer==0.9.8->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.9.8->vllm) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.9.8->vllm) (6.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (3.1.3)\n",
            "Collecting lark (from outlines==0.0.34->vllm)\n",
            "  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (2.2.1)\n",
            "Collecting diskcache (from outlines==0.0.34->vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.11.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.58.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.4.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.35.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (4.19.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0->vllm) (2023.12.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->vllm)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0->vllm)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->vllm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (2.18.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2024.2.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->vllm) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->vllm) (4.66.2)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->vllm)\n",
            "  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi->vllm)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart>=0.0.7 (from fastapi->vllm)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->vllm)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->vllm)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->vllm)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]->vllm)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->vllm) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->vllm) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->vllm) (1.2.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->vllm)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->vllm)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx>=0.23.0->fastapi->vllm)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.34->vllm) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (0.18.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.34->vllm) (0.41.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->vllm) (1.3.0)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (0.1.2)\n",
            "Building wheels for collected packages: flash-attn, vllm-nccl-cu12\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120853537 sha256=53979129f883680327bf5d13027cd014e2d054f4fb5b8856916686ae315e57d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n",
            "  Building wheel for vllm-nccl-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vllm-nccl-cu12: filename=vllm_nccl_cu12-2.18.1.0.4.0-py3-none-any.whl size=5418 sha256=a6a9b47a4fc0e757d7dfc0f31af035203b47884b08c21906fde04bde63f18288\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/28/b5/e99e6ea84b08c0bf19a218d408316e55e02ff725d3616fb79d\n",
            "Successfully built flash-attn vllm-nccl-cu12\n",
            "Installing collected packages: vllm-nccl-cu12, nvidia-ml-py, ninja, websockets, uvloop, ujson, triton, shellingham, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, httptools, h11, einops, dnspython, diskcache, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, typer, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, httpx, torch, ray, openai, xformers, outlines, flash-attn, fastapi-cli, fastapi, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.2 flash-attn-2.5.8 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 interegular-0.3.3 lark-1.1.9 lm-format-enforcer-0.9.8 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.550.52 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-1.26.0 orjson-3.10.3 outlines-0.0.34 prometheus-fastapi-instrumentator-7.0.0 python-dotenv-1.0.1 python-multipart-0.0.9 ray-2.20.0 shellingham-1.5.4 starlette-0.37.2 tiktoken-0.6.0 torch-2.3.0 triton-2.3.0 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 vllm-0.4.2 vllm-nccl-cu12-2.18.1.0.4.0 watchfiles-0.21.0 websockets-12.0 xformers-0.0.26.post1\n"
          ]
        }
      ],
      "source": [
        "# If running on colab: install the necessary packages \n",
        "!pip install vllm flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from vllm import SamplingParams, LLM\n",
        "from tqdm import tqdm\n",
        "from enum import Enum\n",
        "from sklearn.metrics import f1_score\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class InferenceMode(Enum):\n",
        "    GREEDY = \"greedy\"\n",
        "    TOP100 = \"top100\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yaFm9gk9Q6n0"
      },
      "outputs": [],
      "source": [
        "# Storing label tokens in case we use different models so we don't run the tokenizer all the time\n",
        "LABEL_TOKENS_DIRPATH = \"/content/drive/MyDrive/CSCI 567 Final Project/label_tokens\"\n",
        "# Where the instructions/prompts are\n",
        "DATA_CONFIGS_DIRPATH = \"/content/drive/MyDrive/CSCI 567 Final Project/configs\"\n",
        "# Where the models are stored\n",
        "MODELS_DOWNLOAD_DIRPATH = \"/content/drive/MyDrive/CSCI 567 Final Project/models\"\n",
        "# Where the predictions are stored\n",
        "PREDS_OUTPUT_DIRPATH = \"/content/drive/MyDrive/CSCI 567 Final Project/preds\"\n",
        "\n",
        "train_dataset_path = \"/content/drive/MyDrive/CSCI 567 Final Project/data/CNN-and-Essays-Datasets/xsum-gen-n=5000-model=llama3-8b.csv\"\n",
        "test_data_dirpath = \"/content/drive/MyDrive/CSCI 567 Final Project/data/CNN-and-Essays-Datasets/xsum-gen-n=5000-model=llama3-8b.csv\"\n",
        "\n",
        "dataset_name = \"xsum\"  # \"daigt\", \"hewlett\"\n",
        "n_labeled = 4\n",
        "# xsum: 9900, daigt: 10000, hewlett: 7000, cnn: 5200, gp2-essays: 2700\n",
        "n_test_samples = 9900\n",
        "inference_mode = InferenceMode.TOP100\n",
        "label_names = [\"0\", \"1\"]\n",
        "model_name = \"llama3-8b\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb97mDl6Q1Oh"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVKcnnkPQw5x"
      },
      "outputs": [],
      "source": [
        "def format_one_example_for_inference(d: dict, prompt_template: str, label=None):\n",
        "    return prompt_template.replace(\"<TEXT>\", d[\"text\"])\n",
        "\n",
        "\n",
        "def format_one_labeled_example(d: dict, prompt_template: str):\n",
        "    return prompt_template.replace(\"<TEXT>\", d[\"text\"]).replace(\"<LABEL>\", str(d[\"label\"]))\n",
        "\n",
        "\n",
        "def generate_inference_examples(\n",
        "    test_data: list[dict],\n",
        "    prompt_template: str,\n",
        ") -> list[str]:\n",
        "    \"\"\"Generate prompts for inference\n",
        "    Args:\n",
        "        data: list[dict]\n",
        "            The data to generate prompts for\n",
        "        inference_mode: InferenceMode\n",
        "            The inference mode to use\n",
        "    \"\"\"\n",
        "    return [format_one_example_for_inference(item, prompt_template) for item in test_data]\n",
        "\n",
        "\n",
        "def load_demontration_prompt_template(data_config_filepath: str) -> str:\n",
        "    \"\"\"Load the demonstration prompt template from the data config file\n",
        "    Args:\n",
        "        data_config_filepath: str\n",
        "            The path to the data config JSON file\n",
        "    Returns:\n",
        "        str\n",
        "            The demonstration prompt template\n",
        "    \"\"\"\n",
        "    with open(data_config_filepath, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    return config[\"demonstration_prompt\"]\n",
        "\n",
        "\n",
        "def load_inference_prompt_template(data_config_filepath: str) -> str:\n",
        "    \"\"\"Load the inference prompt template from the data config file\n",
        "    Args:\n",
        "        data_config_filepath: str\n",
        "            The path to the data config JSON file\n",
        "    Returns:\n",
        "        str\n",
        "            The inference prompt template\n",
        "    \"\"\"\n",
        "    with open(data_config_filepath, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    return config[\"inference_prompt\"]\n",
        "\n",
        "\n",
        "def format_prefix(labeled_data: list[dict], task_instruction: str, prompt_template: str) -> str:\n",
        "    \"\"\"Format the prefix for the prompt, which contains labeled examples.\n",
        "    Args:\n",
        "        labeled_data: list[dict]\n",
        "            The labeled data\n",
        "        prompt_template: str\n",
        "            The prompt template for labeled exampless\n",
        "    Returns:\n",
        "        str\n",
        "            The formatted prefix\n",
        "    \"\"\"\n",
        "    if not labeled_data:  # For zero-shot learning\n",
        "        return \"\"\n",
        "\n",
        "    prefix = [task_instruction]\n",
        "\n",
        "    for item in labeled_data:\n",
        "        prefix.append(format_one_labeled_example(item, prompt_template))\n",
        "\n",
        "    assert len(prefix) > 0\n",
        "    return \"\\n\".join(prefix)\n",
        "\n",
        "\n",
        "def load_data(\n",
        "    labeled_data_filepath: str,\n",
        "    test_data_filepath: str,\n",
        "    dataset_name: str,\n",
        "    n_labeled: int,\n",
        "    n_test_samples: int,\n",
        "):\n",
        "    \"\"\"Load the data and generate prompts for inference\n",
        "    Args:\n",
        "        labeled_data_filepath: str\n",
        "            The path to the labeled data\n",
        "        test_data_filepath: str\n",
        "            The path to the test data\n",
        "        dataset_name: str\n",
        "            The name of the dataset\n",
        "        n_labeled: int\n",
        "            The number of labeled examples to use\n",
        "        n_test_samples: int\n",
        "            The number of test examples to use\n",
        "    Returns:\n",
        "        tuple[list[str], list[str], list[int]]\n",
        "            The prompts, true labels, and sample ids\n",
        "    \"\"\"\n",
        "    data_config_filepath = f\"{DATA_CONFIGS_DIRPATH}/{dataset_name}_config.json\"\n",
        "    demonstrations = load_file(labeled_data_filepath)[:n_labeled]\n",
        "    if n_test_samples == -1:\n",
        "        test_data = load_file(test_data_filepath)\n",
        "    else:\n",
        "        if labeled_data_filepath == test_data_filepath:\n",
        "            print(\n",
        "                \"Detected same files for training and testing. Picking different subsets in order to not have overlaps\"\n",
        "            )\n",
        "            test_data = load_file(test_data_filepath)[-n_test_samples:]\n",
        "        else:\n",
        "            test_data = load_file(test_data_filepath)[:n_test_samples]\n",
        "\n",
        "    demonstration_prompt_template = load_demontration_prompt_template(data_config_filepath)\n",
        "    inference_prompt_template = load_inference_prompt_template(data_config_filepath)\n",
        "    task_instruction = load_instruction(data_config_filepath)\n",
        "\n",
        "    true_labels = [str(item[\"label\"]) for item in test_data]\n",
        "\n",
        "    # Prefix contains the labeled examples\n",
        "    prefix = format_prefix(demonstrations, task_instruction, demonstration_prompt_template)\n",
        "\n",
        "    test_examples = generate_inference_examples(test_data, inference_prompt_template)\n",
        "\n",
        "    if len(prefix) > 0:  # few-shot case\n",
        "        prompts = [prefix + \"\\n\" + example for example in test_examples]\n",
        "    else:  # zero-shot case\n",
        "        prompts = test_examples\n",
        "\n",
        "    return prompts, true_labels  # , sample_ids\n",
        "\n",
        "\n",
        "def get_label_tokens(model_name, model_path, dataset_name, label_names) -> dict[str, int]:\n",
        "    \"\"\"Get the token ids for the labels in the dataset from a pre-existing file. Generate them and store them if they don't exist.\n",
        "    Args:\n",
        "        model_name: str\n",
        "            The name of the model\n",
        "        model_path: str\n",
        "            The path to the model. Used only if we need to load the tokenizer model.\n",
        "        dataset_name: str\n",
        "            The name of the dataset\n",
        "        label_names: list[str]\n",
        "            The names of the labels\n",
        "    Returns:\n",
        "        dict[str, int]\n",
        "            A map from a label to its token id\n",
        "    \"\"\"\n",
        "    label_tokens_filepath = f\"{LABEL_TOKENS_DIRPATH}/{model_name}_{dataset_name}_label_tokens.json\"\n",
        "    if os.path.exists(label_tokens_filepath):\n",
        "        with open(label_tokens_filepath, \"r\") as f:\n",
        "            label_tokens = json.load(f)\n",
        "            print(f\"Loaded label tokens: {label_tokens}\")\n",
        "    else:\n",
        "        print(f\"Label tokens file not found, generating for model: {model_name}\")\n",
        "        hf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Encode each label and skip the first token because it's the prefix space\n",
        "        label_tokens = {label: hf_tokenizer.encode(label)[1:] for label in label_names}\n",
        "\n",
        "        print(f\"Generated file tokens: {label_tokens}\")\n",
        "\n",
        "        # Store them in a dictionary for easy access\n",
        "        with open(label_tokens_filepath, \"w\") as f:\n",
        "            json.dump(label_tokens, f)\n",
        "        print(f\"Stored file tokens to: {label_tokens_filepath}\")\n",
        "\n",
        "    return label_tokens\n",
        "\n",
        "\n",
        "def load_instruction(data_config_filepath: str) -> str:\n",
        "    \"\"\"Load the task instruction from the data config file\n",
        "    Args:\n",
        "        data_config_filepath: str\n",
        "            The path to the data config JSON file\n",
        "    Returns:\n",
        "        str\n",
        "            The task instruction that comes before the examples in the prompt\n",
        "    \"\"\"\n",
        "    with open(data_config_filepath, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    return config[\"instruction\"]\n",
        "\n",
        "\n",
        "def load_file(filename: str) -> list[dict]:\n",
        "    \"\"\"Load a file into a list of dictionaries\n",
        "    Args:\n",
        "        filename: str\n",
        "            The path to the file\n",
        "    Returns:\n",
        "        list[dict]\n",
        "            The list of data points as dictionaries\n",
        "    \"\"\"\n",
        "    print(f\"Loading: {filename}...\")\n",
        "    if filename.endswith(\".jsonl\"):\n",
        "        data = pd.read_json(filename, orient=\"records\", encoding=\"utf-8\").to_dict(\"records\")\n",
        "    elif filename.endswith(\".csv\"):\n",
        "        data = pd.read_csv(filename, encoding=\"utf-8\").to_dict(\"records\")\n",
        "\n",
        "    print(f\"Detected: {len(data)} rows.\")\n",
        "    if \"generated\" in data[0]:\n",
        "        for datapoint in data:\n",
        "            datapoint[\"label\"] = datapoint[\"generated\"]\n",
        "\n",
        "    data0 = [d for d in data if d[\"label\"] == 0]\n",
        "    data0 = sorted(data0, key=lambda kv: len(kv[\"text\"].split()))\n",
        "    data1 = [d for d in data if d[\"label\"] == 1]\n",
        "    data1 = sorted(data1, key=lambda kv: len(kv[\"text\"].split()))\n",
        "\n",
        "    print(\n",
        "        f\"Label split: {len(data0) / len(data)} human-written, {len(data1) / len(data)} AI-generated\"\n",
        "    )\n",
        "\n",
        "    all_data = []\n",
        "    for d0, d1 in zip(data0, data1):\n",
        "        all_data.append(d0)\n",
        "        all_data.append(d1)\n",
        "\n",
        "    # In an imbalanced dataset the previous loop will only run for the length of the shortest list, so we add this to add the remaining samples\n",
        "    if len(data0) != len(data1):\n",
        "        if len(data0) < len(data1):\n",
        "            smol = data0\n",
        "            huge = data1\n",
        "        elif len(data0) > len(data1):\n",
        "            smol = data1\n",
        "            huge = data0\n",
        "\n",
        "        for i in range(len(smol), len(huge)):\n",
        "            all_data.append(huge[i])\n",
        "\n",
        "    # NOTE: We're hardcoding this for a dataset we know contains extremely long essays to not run into a context length error\n",
        "    if \"CNN Dataset.csv\" in filename or \"Essays Dataset.csv\" in filename:\n",
        "        all_data = all_data[:5210]\n",
        "\n",
        "    print(f\"Final number of datapoints: {len(all_data)=}\")\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU_in1fYRIAG",
        "outputId": "3e4de843-4925-46c4-83b3-cbd9a1fc15f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading: /content/drive/MyDrive/CSCI 567 Final Project/data/CNN-and-Essays-Datasets/CNN Dataset.csv...\n",
            "Detected: 5330 rows.\n",
            "Label split: 0.5628517823639775 human-written, 0.4371482176360225 AI-generated\n",
            "Final number of datapoints: len(all_data)=5210\n",
            "Detected same files for training and testing. Picking different subsets in order to not have overlaps\n",
            "Loading: /content/drive/MyDrive/CSCI 567 Final Project/data/CNN-and-Essays-Datasets/CNN Dataset.csv...\n",
            "Detected: 5330 rows.\n",
            "Label split: 0.5628517823639775 human-written, 0.4371482176360225 AI-generated\n",
            "Final number of datapoints: len(all_data)=5210\n"
          ]
        }
      ],
      "source": [
        "prompts, true_labels = load_data(\n",
        "    labeled_data_filepath=train_dataset_path,\n",
        "    test_data_filepath=test_data_dirpath,\n",
        "    dataset_name=dataset_name,\n",
        "    n_labeled=n_labeled,\n",
        "    n_test_samples=n_test_samples,\n",
        "    inference_mode=inference_mode,\n",
        "    label_names=label_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2r7wVkccXRt"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gRsQSd-Ia0me"
      },
      "outputs": [],
      "source": [
        "def write_ood_and_missing_labels_to_file(\n",
        "    preds_output_filepath, ood_predictions, labels_notin_top100\n",
        "):\n",
        "    preds_output_filepath = preds_output_filepath.replace(\".jsonl\", \"-ood-missing-labels.txt\")\n",
        "\n",
        "    # Store ood predictions in the same predictions file\n",
        "    if ood_predictions:\n",
        "        with open(preds_output_filepath, \"a\") as f:\n",
        "            f.write(f\"{'-' * 80}\\n\")\n",
        "            f.write(\"Out-of-distribution predictions:\\n\")\n",
        "            for i, pred in ood_predictions:\n",
        "                f.write(f\"i: {i}, ood_pred: {pred}\\n\")\n",
        "            f.write(f\"{'-' * 80}\\n\")\n",
        "\n",
        "    # Store number of labels not in the top 100 tokens in the same predictions file\n",
        "    if labels_notin_top100:\n",
        "        with open(preds_output_filepath, \"a\") as f:\n",
        "            f.write(f\"{'-' * 80}\\n\")\n",
        "            f.write(\"Labels not in top 100 tokens:\\n\")\n",
        "            for i, label in labels_notin_top100:\n",
        "                f.write(f\"pred idx: {i}, label: {label}\\n\")\n",
        "            f.write(f\"{'-' * 80}\\n\")\n",
        "\n",
        "\n",
        "def do_inference(\n",
        "    inference_mode: InferenceMode,\n",
        "    llm: LLM,\n",
        "    sampling_params: SamplingParams,\n",
        "    prompts: list[str],\n",
        "    preds_output_filepath: str,\n",
        "    true_labels: list[str],\n",
        "    label_tokens: dict[str, list[int]],\n",
        ") -> tuple[list[str], list[int], list[tuple[int, str]], list[tuple[int, str]]]:\n",
        "    \"\"\"Perform inference on the model.\n",
        "    Args:\n",
        "        inference_mode: InferenceMode\n",
        "            The mode of inference\n",
        "        llm: LLM\n",
        "            The language model\n",
        "        sampling_params: SamplingParams\n",
        "            The sampling parameters\n",
        "        prompts: list[str]\n",
        "            The prompts for the model\n",
        "        preds_output_filepath: str\n",
        "            The path to the file where the predictions will be written\n",
        "        true_labels: list[str]\n",
        "            The true labels\n",
        "        label_tokens: dict[str, list[int]]\n",
        "            A map from a label to its token ids\n",
        "    Returns:\n",
        "        tuple[list[str], list[str], list[tuple[int, str]], list[tuple[int, str]]]\n",
        "            The predictions, true labels, labels not in the top 100 tokens, and OOD predictions\n",
        "    \"\"\"\n",
        "    label2idx = {label: idx for idx, label in enumerate(list(label_tokens.keys()))}\n",
        "\n",
        "    print(\"Generating outputs...\")\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "    predictions = []\n",
        "    ood_predictions = []\n",
        "    labels_notin_top100 = []\n",
        "\n",
        "    if inference_mode == InferenceMode.GREEDY:\n",
        "        print(\"Greedy inference\")\n",
        "\n",
        "        for i, output in enumerate(tqdm(outputs)):\n",
        "            generated_text = output.outputs[0].text.strip()\n",
        "            if generated_text not in label2idx:\n",
        "                ood_predictions.append((i, generated_text))\n",
        "\n",
        "            # Take into account case where instead of the model generates \"\\n\" instead of \"Input:\", as if it were to write a new example\n",
        "            if \"Input:\" in generated_text:\n",
        "                pred = generated_text.split(\"Input:\")[0]\n",
        "            else:\n",
        "                pred = generated_text\n",
        "\n",
        "            pred_obj = {\n",
        "                \"generated_text\": generated_text,\n",
        "                \"pred\": pred,\n",
        "                \"true_label\": true_labels[i],\n",
        "            }\n",
        "\n",
        "            with open(preds_output_filepath, \"a\") as f:\n",
        "                f.write(json.dumps(pred_obj) + \"\\n\")\n",
        "\n",
        "            predictions.append(pred)\n",
        "\n",
        "        print(f\"Number of OOD predictions: {len(ood_predictions)}\")\n",
        "\n",
        "        write_ood_and_missing_labels_to_file(\n",
        "            preds_output_filepath, ood_predictions, labels_notin_top100\n",
        "        )\n",
        "\n",
        "    elif inference_mode == InferenceMode.TOP100:\n",
        "        print(\"Top100 inference\")\n",
        "\n",
        "        for i, output in enumerate(tqdm(outputs)):\n",
        "            generated_text = output.outputs[0].text\n",
        "            logprobs = get_logprobs_from_output(output.outputs[0], label_tokens)\n",
        "            label_probs, missing_labels = get_label_probs(logprobs, label2idx)\n",
        "            if missing_labels:\n",
        "                if len(missing_labels) == len(label2idx):\n",
        "                    ood_predictions.append((i, generated_text))\n",
        "                for missing_label in missing_labels:\n",
        "                    labels_notin_top100.append((i, missing_label))\n",
        "\n",
        "            idx2label = {v: k for k, v in label2idx.items()}\n",
        "            pred = idx2label[np.argmax(label_probs)].replace(\"_\", \"\")\n",
        "            pred_obj = {\n",
        "                \"generated_text\": generated_text,\n",
        "                \"pred\": pred,\n",
        "                \"true_label\": true_labels[i],\n",
        "                # \"id\": sample_ids[i],\n",
        "                \"logprobs\": logprobs,\n",
        "            }\n",
        "\n",
        "            with open(preds_output_filepath, \"a\") as f:\n",
        "                f.write(json.dumps(pred_obj) + \"\\n\")\n",
        "\n",
        "            predictions.append(pred)\n",
        "\n",
        "        print(\n",
        "            f\"Number of OOD predictions: {len(ood_predictions)}, number of labels not in top 100 tokens: {len(labels_notin_top100)}\"\n",
        "        )\n",
        "\n",
        "        write_ood_and_missing_labels_to_file(\n",
        "            preds_output_filepath, ood_predictions, labels_notin_top100\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid inference mode: {inference_mode}\")\n",
        "\n",
        "    return predictions, true_labels, labels_notin_top100, ood_predictions\n",
        "\n",
        "\n",
        "def get_sampling_params(inference_mode: InferenceMode):\n",
        "    \"\"\"Get the sampling parameters for the model.\n",
        "    Args:\n",
        "        inference_mode: InferenceMode\n",
        "            The mode of inference\n",
        "    Returns:\n",
        "        SamplingParams\n",
        "            The sampling parameters\n",
        "    \"\"\"\n",
        "    if inference_mode == InferenceMode.GREEDY:\n",
        "        params = SamplingParams(temperature=0.0, stop=\"\\n\")\n",
        "    elif inference_mode == InferenceMode.TOP100:\n",
        "        params = SamplingParams(temperature=0.0, logprobs=100, max_tokens=1)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid inference mode: {inference_mode}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_logprobs_from_output(output, label_tokens):\n",
        "    \"\"\"Get the logprobs and the probs of the labels from the output of the LLM model.\n",
        "    Args:\n",
        "        output: CompletionOutput\n",
        "            The output of the LLM model for a single example\n",
        "        label_tokens: dict[str, int]\n",
        "            A map from a label to its token id\n",
        "    Returns:\n",
        "        dict[str, tuple]\n",
        "            A map from a label to a tuple of (logprob, prob, index in top 100 tokens)\n",
        "    \"\"\"\n",
        "    log_probs = output.logprobs\n",
        "    # Get the logprobs of the labels\n",
        "    label_logprobs = {}\n",
        "    for label, label_tokens in label_tokens.items():\n",
        "        # We're only interested in the first token of the label, since this should only be called for single token labels\n",
        "        label_token = label_tokens[0]\n",
        "        # We're iterating through the dictionary because it is in descending order of logprob, so we can also get the index\n",
        "        for i, token in enumerate(log_probs[0].keys()):\n",
        "            if label_token == token:\n",
        "                label_logprobs[label] = (\n",
        "                    log_probs[0][label_token].logprob,\n",
        "                    np.exp(log_probs[0][label_token].logprob),\n",
        "                    i,\n",
        "                )  # logprob, index in top 100 tokens\n",
        "\n",
        "    return dict(sorted(label_logprobs.items(), key=lambda kv: kv[1][-1]))\n",
        "\n",
        "\n",
        "def get_label_logprobs_from_prompt(label_token_values, prompt_logprobs) -> list[tuple]:\n",
        "    \"\"\"Get the logprobs of the labels from the prompt logprobs.\n",
        "    Args:\n",
        "        label_token_values: list[int]\n",
        "            The token values of a given label\n",
        "        prompt_logprobs: list[dict[int, float]]\n",
        "            A list of dictionaries, where each dictionary contains a map from a token to its logprob\n",
        "    Returns:\n",
        "        list[tuple]\n",
        "            A list of tuples, where each tuple contains a label token and its logprob\n",
        "    \"\"\"\n",
        "    label_logprobs = []  # [(token: logprob)]\n",
        "\n",
        "    # Get the last N elements of the prompt logprobs list, where N is the number of tokens in the word we're looking for\n",
        "    n_last_logprobs = prompt_logprobs[-len(label_token_values) :]\n",
        "    # print(f\"Prompt logprobs: {n_last_logprobs}\")\n",
        "    for i, tokendict in enumerate(n_last_logprobs):\n",
        "\n",
        "        # Sanity check, technically we should never hit this\n",
        "        if label_token_values[i] not in tokendict:\n",
        "            print(f\"{i=}\")\n",
        "            print(f\"Prompt logprobs: {tokendict}\")\n",
        "            print(f\"Label tokens: {label_token_values}\")\n",
        "            raise ValueError(f\"Token {label_token_values[i]} not found in the prompt logprobs\")\n",
        "\n",
        "        label_logprobs.append((label_token_values[i], tokendict[label_token_values[i]]))\n",
        "\n",
        "    return label_logprobs\n",
        "\n",
        "\n",
        "def get_label_probs(logprobs, label2idx):\n",
        "    \"\"\"Get the non-normalized predicted probabilities for each label.\n",
        "    Args:\n",
        "        logprobs: dict[str, tuple]\n",
        "            A map from a label to a tuple of (logprob, prob, index in top 100 tokens)\n",
        "        label2idx: dict[str, int]\n",
        "            A map from a label to its index in the list of labels.\n",
        "    Returns:\n",
        "        list[list[float]], list[tuple]\n",
        "            A list of lists, where each list contains the non-normalized predicted probabilities for each label.\n",
        "            Also returns a list of tuples containing the indices of the test samples that had missing labels, plus the missing label.\n",
        "    \"\"\"\n",
        "    missing_labels = []\n",
        "\n",
        "    # Get the probs only and ensure that all labels are within the top 100 tokens in the prediction\n",
        "    pred_probs = [0] * len(label2idx)\n",
        "    for label in label2idx:\n",
        "        if label in logprobs:\n",
        "            pred_probs[label2idx[label]] = logprobs[label][1]  # non-normalized\n",
        "        else:\n",
        "            # print(f\"Label not found: {label}\")\n",
        "            missing_labels.append(label)\n",
        "\n",
        "    return pred_probs, missing_labels\n",
        "\n",
        "\n",
        "def get_model_config(model_name: str):\n",
        "    \"\"\"Get the configuration for the model.\n",
        "    Args:\n",
        "        model_name: str\n",
        "            The name of the model\n",
        "    Returns:\n",
        "        dict\n",
        "            The configuration for the model\n",
        "    \"\"\"\n",
        "    model_config = {}\n",
        "    if model_name == \"mixtral-8x7b\":\n",
        "        # gpu_memory_utilization=0.6, tensor_parallel_size=4\n",
        "        model_config = {\n",
        "            \"model\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "            \"gpu_memory_utilization\": 0.6,\n",
        "            \"tensor_parallel_size\": 4,\n",
        "        }\n",
        "    elif model_name == \"gemma-7b\":\n",
        "        # gpu_memory_utilization=0.45, max_model_len=4096\n",
        "        model_config = {\n",
        "            \"model\": \"google/gemma-7b\",\n",
        "            \"gpu_memory_utilization\": 0.45,\n",
        "            \"max_model_len\": 4096,\n",
        "        }\n",
        "    elif model_name == \"mistral-7b\":\n",
        "        # gpu_memory_utilization=0.7\n",
        "        model_config = {\n",
        "            \"model\": \"mistralai/Mistral-7B-v0.1\",\n",
        "            \"gpu_memory_utilization\": 0.7,\n",
        "        }\n",
        "    elif model_name == \"LLaMA2-13B\":\n",
        "        # tensor_parallel_size=2\n",
        "        model_config = {\n",
        "            \"model\": \"meta-llama/Llama-2-13b-hf\",\n",
        "            \"tensor_parallel_size\": 2,\n",
        "        }\n",
        "    elif model_name == \"LLaMA2-70B\":\n",
        "        # gpu_memory_utilization=0.85, tensor_parallel_size=4\n",
        "        model_config = {\n",
        "            \"model\": \"meta-llama/Llama-2-70b-hf\",\n",
        "            \"gpu_memory_utilization\": 0.9,\n",
        "            \"tensor_parallel_size\": 4,\n",
        "        }\n",
        "    elif model_name == \"LLaMA2-7B\":\n",
        "        model_config = {\n",
        "            \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
        "        }\n",
        "    elif model_name == \"llama3-8b\":\n",
        "        model_config = {\n",
        "            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n",
        "        }\n",
        "    elif model_name == \"mistral-7b-instruct-v2\":\n",
        "        model_config = {\n",
        "            \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        }\n",
        "    elif model_name == \"llama3-8b-instruct\":\n",
        "        model_config = {\n",
        "            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Model name not recognized: {model_name}\")\n",
        "\n",
        "    model_config[\"download_dir\"] = MODELS_DOWNLOAD_DIRPATH\n",
        "    model_config[\"max_logprobs\"] = 100\n",
        "    return model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "3db87f11a8724b5aacfda2e40acd8d32",
            "06705f1dc33344a98bc81c7dc56bc0f6",
            "64478db132e04273a5b875e5471cb3fc",
            "f93b49bcb4a245aab0818983a6634243",
            "011d28a4ae93416c8cd563fc7c1615df",
            "8d83aef4b1f648d98e9bda4cccc50cfd",
            "8eaffefe6bf8459189e9fd56256242b3",
            "67a9a99a1b234dc5b2a8f80087aa0fab",
            "4bc140f5a64d41b0946120b7032d6e29",
            "0a633a1897574823832e60400451daf8",
            "19ee9895b78a48ddada952db5dcd023f",
            "6dca5c98f4e54741af88f938f2e21269",
            "048e05193f5c44c8a1a28223a69f0acc",
            "8f13fc9dc86345a8971d3d6001ce8ea6",
            "7bb82c4eb4d54efcbb65be7581373f30",
            "3aa56eaa06844f1fb1a8ff082ea05b80",
            "56434cb0f6cd48718564b635fa0453bf",
            "8a868815e0214f9caf8f41c4d090d403",
            "7cd6053a21564883bab7490484c41b6a",
            "c6c0ce275ff248078064d55cc6a5afba",
            "57225b31b02941e8a430a9a61c98ab57",
            "8125c0955b2e44ae90cbac9bb43d59f4",
            "f8a071e7865045d0ac7eb76dfab0f8fa",
            "cb7d2046237549be991653ee85c034e4",
            "fad82788d55d440e99089510d7fdc9f9",
            "28e6951a2b994b74889d740e5e4ba9e5",
            "7610d27bec6a437bbe3907c22fdda8b6",
            "c9fd8469ccb0473095b275836c10a2dd",
            "4fed3206fef54819bf2352f67df479d1",
            "014ad2e3627b44ac8888fcc557e1dccb",
            "083c805da3a84620ae8eca6a8637b41f",
            "9c851e690a174ef58fa344876e4d8dd9",
            "69c151f78fc244b1b6c1d5b056adf109",
            "a159a2201c024453a567dc18e5136b34",
            "84ebd6ed40f840a4945bf468f9dafb17",
            "2f3ffd82f78a473c8885a976f7db6fad",
            "fd8ff3fd855040a781bbde4d9089450d",
            "cb36608d16e84f6982d24b63ff23c438",
            "95ee80e1d36d46168b41fad0dfc392f4",
            "731cccfbd7a14022b07681dc1ef64cfb",
            "a1ce121291054b20bdb09691b586487f",
            "0f78a5f871d1428ea61335519ef21021",
            "976b95b1a7984174a19bdd6daae16bf8",
            "99b79a2dd49c4ab6937151d819b68e54",
            "e1e1f8ba7c3344ecb3474e53d6ee1566",
            "5a925ebbe5e24fa39e7107d34891eae5",
            "d4da17a806bc4b67b282d2922b0309c9",
            "118b1495b47042649e1cf746ec8d8786",
            "8e340050d3af45cab241a3769186125d",
            "7e6e5ecc604b473eae1168a65b7cb648",
            "347e7a3bfb074e439a801f6af0f4f5c6",
            "824adb7d287145b7a113c780b9651d0d",
            "28339ea8e3684159baf21a6b6e4992d2",
            "28ddc819cbe94e948f32a821d6951d53",
            "dd4acd6309c74dc08c66d6dcc1062d0d"
          ]
        },
        "id": "qQ0oZWNFdAPD",
        "outputId": "1c2f55e0-296a-404f-cb39-d5a11cfba18b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3db87f11a8724b5aacfda2e40acd8d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-06 21:58:33 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/content/drive/MyDrive/CSCI 567 Final Project/models', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dca5c98f4e54741af88f938f2e21269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8a071e7865045d0ac7eb76dfab0f8fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a159a2201c024453a567dc18e5136b34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1e1f8ba7c3344ecb3474e53d6ee1566",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-06 21:58:37 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
            "INFO 05-06 21:58:38 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
            "INFO 05-06 21:58:38 selector.py:32] Using XFormers backend.\n",
            "INFO 05-06 21:58:40 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
            "INFO 05-06 22:04:59 model_runner.py:175] Loading model weights took 14.9595 GB\n",
            "INFO 05-06 22:05:00 gpu_executor.py:114] # GPU blocks: 9557, # CPU blocks: 2048\n",
            "INFO 05-06 22:05:02 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 05-06 22:05:02 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 05-06 22:05:10 model_runner.py:1017] Graph capturing finished in 8 secs.\n"
          ]
        }
      ],
      "source": [
        "model_config = get_model_config(model_name)\n",
        "sampling_params = get_sampling_params(inference_mode)\n",
        "\n",
        "llm = LLM(**model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2SV5RUfGJs",
        "outputId": "0c91c1e8-6eab-4e4f-c072-4aebaeeba2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded label tokens: {'0': [15], '1': [16]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'0': [15], '1': [16]}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_tokens = get_label_tokens(model_name, model_config[\"model\"], dataset_name, label_names)\n",
        "label_tokens = dict(sorted(label_tokens.items(), key=lambda x: x[0]))\n",
        "\n",
        "label_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcvwX5p0c6wr",
        "outputId": "5e29a7ad-70de-4a24-fd1c-c197252e1ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating outputs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 5200/5200 [05:45<00:00, 15.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top100 inference\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5200/5200 [00:18<00:00, 274.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of OOD predictions: 1, number of labels not in top 100 tokens: 2\n"
          ]
        }
      ],
      "source": [
        "preds_output_filepath = f\"{PREDS_OUTPUT_DIRPATH}/test-preds-dataset={test_data_dirpath.split('/')[-1].replace('.csv', '')}-model={model_name}.jsonl\"\n",
        "predictions, true_labels, labels_notin_top100, ood_predictions = do_inference(\n",
        "    inference_mode=inference_mode,\n",
        "    llm=llm,\n",
        "    sampling_params=sampling_params,\n",
        "    prompts=prompts,\n",
        "    preds_output_filepath=preds_output_filepath,\n",
        "    true_labels=true_labels,\n",
        "    sample_ids=None,\n",
        "    label_tokens=label_tokens,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rVAamzOEgVfe"
      },
      "outputs": [],
      "source": [
        "def evaluate(true_labels, predictions):\n",
        "    print(\"Evaluating...\")\n",
        "    uncalibrated_acc = np.mean(\n",
        "        [1 if pred == true_label else 0 for pred, true_label in zip(predictions, true_labels)]\n",
        "    )\n",
        "    micro_f1 = f1_score(true_labels, predictions, average=\"micro\")\n",
        "    weighted_f1 = f1_score(true_labels, predictions, average=\"weighted\")\n",
        "\n",
        "    print(f\"Uncalibrated accuracy: {uncalibrated_acc:.4f}\")\n",
        "    print(f\"Uncalibrated micro F1 score: {micro_f1:.4f}\")\n",
        "    print(f\"Uncalibrated weighted F1 score: {weighted_f1:.4f}\")\n",
        "    return uncalibrated_acc, micro_f1, weighted_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzLTCG50gWbB",
        "outputId": "cee4532e-d0d7-4dfb-df38-d514dea63a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "Uncalibrated accuracy: 0.5465\n",
            "Uncalibrated micro F1 score: 0.5465\n",
            "Uncalibrated weighted F1 score: 0.5476\n"
          ]
        }
      ],
      "source": [
        "uncalibrated_acc, micro_f1, weighted_f1 = evaluate(true_labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR2C02tifRpu",
        "outputId": "0b0a1c60-1b85-4af2-fc84-339c8a9731a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'0': 2719, '1': 2481})"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njth_PCFjuCU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "011d28a4ae93416c8cd563fc7c1615df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014ad2e3627b44ac8888fcc557e1dccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048e05193f5c44c8a1a28223a69f0acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56434cb0f6cd48718564b635fa0453bf",
            "placeholder": "​",
            "style": "IPY_MODEL_8a868815e0214f9caf8f41c4d090d403",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "06705f1dc33344a98bc81c7dc56bc0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d83aef4b1f648d98e9bda4cccc50cfd",
            "placeholder": "​",
            "style": "IPY_MODEL_8eaffefe6bf8459189e9fd56256242b3",
            "value": "config.json: 100%"
          }
        },
        "083c805da3a84620ae8eca6a8637b41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a633a1897574823832e60400451daf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f78a5f871d1428ea61335519ef21021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "118b1495b47042649e1cf746ec8d8786": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ddc819cbe94e948f32a821d6951d53",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4acd6309c74dc08c66d6dcc1062d0d",
            "value": " 177/177 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "19ee9895b78a48ddada952db5dcd023f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28339ea8e3684159baf21a6b6e4992d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28ddc819cbe94e948f32a821d6951d53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e6951a2b994b74889d740e5e4ba9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c851e690a174ef58fa344876e4d8dd9",
            "placeholder": "​",
            "style": "IPY_MODEL_69c151f78fc244b1b6c1d5b056adf109",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 12.9MB/s]"
          }
        },
        "2f3ffd82f78a473c8885a976f7db6fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ce121291054b20bdb09691b586487f",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f78a5f871d1428ea61335519ef21021",
            "value": 73
          }
        },
        "347e7a3bfb074e439a801f6af0f4f5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aa56eaa06844f1fb1a8ff082ea05b80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db87f11a8724b5aacfda2e40acd8d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06705f1dc33344a98bc81c7dc56bc0f6",
              "IPY_MODEL_64478db132e04273a5b875e5471cb3fc",
              "IPY_MODEL_f93b49bcb4a245aab0818983a6634243"
            ],
            "layout": "IPY_MODEL_011d28a4ae93416c8cd563fc7c1615df"
          }
        },
        "4bc140f5a64d41b0946120b7032d6e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fed3206fef54819bf2352f67df479d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56434cb0f6cd48718564b635fa0453bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57225b31b02941e8a430a9a61c98ab57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a925ebbe5e24fa39e7107d34891eae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e6e5ecc604b473eae1168a65b7cb648",
            "placeholder": "​",
            "style": "IPY_MODEL_347e7a3bfb074e439a801f6af0f4f5c6",
            "value": "generation_config.json: 100%"
          }
        },
        "64478db132e04273a5b875e5471cb3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67a9a99a1b234dc5b2a8f80087aa0fab",
            "max": 654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bc140f5a64d41b0946120b7032d6e29",
            "value": 654
          }
        },
        "67a9a99a1b234dc5b2a8f80087aa0fab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c151f78fc244b1b6c1d5b056adf109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dca5c98f4e54741af88f938f2e21269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_048e05193f5c44c8a1a28223a69f0acc",
              "IPY_MODEL_8f13fc9dc86345a8971d3d6001ce8ea6",
              "IPY_MODEL_7bb82c4eb4d54efcbb65be7581373f30"
            ],
            "layout": "IPY_MODEL_3aa56eaa06844f1fb1a8ff082ea05b80"
          }
        },
        "731cccfbd7a14022b07681dc1ef64cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7610d27bec6a437bbe3907c22fdda8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb82c4eb4d54efcbb65be7581373f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57225b31b02941e8a430a9a61c98ab57",
            "placeholder": "​",
            "style": "IPY_MODEL_8125c0955b2e44ae90cbac9bb43d59f4",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 4.25MB/s]"
          }
        },
        "7cd6053a21564883bab7490484c41b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6e5ecc604b473eae1168a65b7cb648": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8125c0955b2e44ae90cbac9bb43d59f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824adb7d287145b7a113c780b9651d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ebd6ed40f840a4945bf468f9dafb17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ee80e1d36d46168b41fad0dfc392f4",
            "placeholder": "​",
            "style": "IPY_MODEL_731cccfbd7a14022b07681dc1ef64cfb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8a868815e0214f9caf8f41c4d090d403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d83aef4b1f648d98e9bda4cccc50cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e340050d3af45cab241a3769186125d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eaffefe6bf8459189e9fd56256242b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f13fc9dc86345a8971d3d6001ce8ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd6053a21564883bab7490484c41b6a",
            "max": 50566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6c0ce275ff248078064d55cc6a5afba",
            "value": 50566
          }
        },
        "95ee80e1d36d46168b41fad0dfc392f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "976b95b1a7984174a19bdd6daae16bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b79a2dd49c4ab6937151d819b68e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c851e690a174ef58fa344876e4d8dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a159a2201c024453a567dc18e5136b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84ebd6ed40f840a4945bf468f9dafb17",
              "IPY_MODEL_2f3ffd82f78a473c8885a976f7db6fad",
              "IPY_MODEL_fd8ff3fd855040a781bbde4d9089450d"
            ],
            "layout": "IPY_MODEL_cb36608d16e84f6982d24b63ff23c438"
          }
        },
        "a1ce121291054b20bdb09691b586487f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6c0ce275ff248078064d55cc6a5afba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9fd8469ccb0473095b275836c10a2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb36608d16e84f6982d24b63ff23c438": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb7d2046237549be991653ee85c034e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fd8469ccb0473095b275836c10a2dd",
            "placeholder": "​",
            "style": "IPY_MODEL_4fed3206fef54819bf2352f67df479d1",
            "value": "tokenizer.json: 100%"
          }
        },
        "d4da17a806bc4b67b282d2922b0309c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_824adb7d287145b7a113c780b9651d0d",
            "max": 177,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28339ea8e3684159baf21a6b6e4992d2",
            "value": 177
          }
        },
        "dd4acd6309c74dc08c66d6dcc1062d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e1f8ba7c3344ecb3474e53d6ee1566": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a925ebbe5e24fa39e7107d34891eae5",
              "IPY_MODEL_d4da17a806bc4b67b282d2922b0309c9",
              "IPY_MODEL_118b1495b47042649e1cf746ec8d8786"
            ],
            "layout": "IPY_MODEL_8e340050d3af45cab241a3769186125d"
          }
        },
        "f8a071e7865045d0ac7eb76dfab0f8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb7d2046237549be991653ee85c034e4",
              "IPY_MODEL_fad82788d55d440e99089510d7fdc9f9",
              "IPY_MODEL_28e6951a2b994b74889d740e5e4ba9e5"
            ],
            "layout": "IPY_MODEL_7610d27bec6a437bbe3907c22fdda8b6"
          }
        },
        "f93b49bcb4a245aab0818983a6634243": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a633a1897574823832e60400451daf8",
            "placeholder": "​",
            "style": "IPY_MODEL_19ee9895b78a48ddada952db5dcd023f",
            "value": " 654/654 [00:00&lt;00:00, 54.2kB/s]"
          }
        },
        "fad82788d55d440e99089510d7fdc9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014ad2e3627b44ac8888fcc557e1dccb",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_083c805da3a84620ae8eca6a8637b41f",
            "value": 9085698
          }
        },
        "fd8ff3fd855040a781bbde4d9089450d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_976b95b1a7984174a19bdd6daae16bf8",
            "placeholder": "​",
            "style": "IPY_MODEL_99b79a2dd49c4ab6937151d819b68e54",
            "value": " 73.0/73.0 [00:00&lt;00:00, 6.68kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
